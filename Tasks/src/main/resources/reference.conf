

###################
# Task subsystem  #
###################

# This subsystem allows jobs to be executed concurrently over a set of
# computers.
# It tracks which job is finished and remembers
# the result across runs. (this is termed "cache" in later on)
# It has a simple file sharing facility in case a shared filesystem is not
# available.
# It has very simple resource definitions (CPU,RAM).
# It supports automatic "elastic" shrinking and growing of the
#  available nodes via LSF, DRMAA, SSH or Amazon AWS.
# It works without these systems if you manually start the workers.
#
# File Sharing
# ============
# The unit of the shared filesystem is a so called SharedFile which has a
# unique name.
# A central storage keeps a record of each of these unique names and a list of
# possible file pathes.
# If a node wants to use the file, then it asks the for the possible pathes
# and tests which path it can read. If there is such a file, then it uses that.
# If there is no readable path, then the file will be transfered from the
# central storage to the node's temporary folder.


hosts.port = 28888

# List of hostnames as supplied by LSF
# This is only needed for LSF, in that case use it as
# -Dhosts.list="$LSB_HOSTS"  from the command line
hosts.list = "localhost"

# number of available CPU's on the current machine
hosts.numCPU = 1

# number of available ram on the current machine in Mb
hosts.RAM = 96000

# number of CPU's reserved for purposes other than Tasks
hosts.reservedCPU = 0

# hostname of the current machine.
hosts.hostname = "localhost"

#LSF or SGE or EC2 or SSH
hosts.gridengine = "NOENGINE"

# advanced. may specify an ip:port where a cacherActor is listening
hosts.remoteCacheAddress = none

# !Important!
# This file has the progress tracking across runs.
# If deleted, most of the progress is lost.
tasks.cacheFile = "cache.leveldb"

tasks.fileServiceFileList = "files.leveldb"

# leave empty to disable file log
tasks.logFile = "run.log"

# writes akka log to stdout
tasks.stdout = true

# Keep track of the progress or not?
tasks.cacheEnabled = true

tasks.cache.store = "filesystem"

tasks.proxytaskGetBackResultTimeout = 28 days

# !Important!
# This is the folder where or valuable result and itermediare files
# are written
tasks.fileService.storageURI = "./"

tasks.fileServiceThreadPoolSize = 8

# advanced. I see no point of having this disabled.
# Cache checks whether a file is still accessible. If not treats it as no hit.
tasks.verifySharedFileInCache = true

# !Important!
# Every input file except those which are found in the below listed folders
# are copied to the tasks.fileServiceBaseFolder .
tasks.fileServiceExtendedFolders = []

# Individuals compute nodes can access the tasks.fileServiceBaseFolder .
# This is a performance tuning, if not then the file copy operations are
# routed trough the master node.
tasks.fileServiceBaseFolderIsShared = false

tasks.nonLocalFileSystems = []

# !Important!
# The file sharing facility works by assigning a unique name to every file.
# If 2 input files whould have the same basename (filename without path) then
# those 2 would clash. This setting will cause to have the full path in the
# unique names of the input files.
tasks.includeFullPathInDefaultSharedName = false

# !Important!
# Resubmit failed tasks for execution.
tasks.resubmitFailedTask = false

# Ping interval between worker and queue.
tasks.askInterval = 100 ms

tasks.disableRemoting = false

tasks.skipContentHashVerificationAfterCache = false

# !Important!
# The automatic "elastic" shrinking growing system is enabled.
tasks.elastic.enabled = false

# Kill a node if it is idle after x time.
tasks.elastic.idleNodeTimeout = 900 sec

# Max nodes to spawn
tasks.elastic.maxNodes = 2

# Max pending nodes. After this is reached no new node request is submitted.
tasks.elastic.maxPending = ${tasks.elastic.maxNodes}

# The elastic subsystem checks the task queue this often.
tasks.elastic.queueCheckInterval = 60 sec

tasks.elastic.queueCheckInitialDelay = 5 sec

tasks.elastic.nodeKillerMonitorInterval = 5 sec

# !Important!
# How many CPUs request for 1 new node.
tasks.elastic.lsf.newNodeSize = 8

# Path to the jar file which will be executed on the new node with
# java -Xmx$MEMm $additionalParameters -jar $path

tasks.elastic.mainClassWithClassPathOrJar = ""

tasks.elastic.lsf.requestedMemOfNewNode = 10000

tasks.elastic.lsf.email = "bartha.pityu@gmail.com"

tasks.elastic.lsf.queue = normal

tasks.elastic.lsf.additionalSystemProperties = []

tasks.elastic.logQueueStatus = true

# The jvmMaxHeapFactor * requestedMemOfNewNode is assigned to -Xmx
tasks.elastic.jvmMaxHeapFactor = 0.7

# This is an object in which every child object represents a node.
# The child objects have the following fields:
# hostname keyFile username memory cpu extraArgs jarPath
# keyFile points to unencrypted passphrase less openssh keyfile.
tasks.elastic.ssh.hosts = {}

# Don't touch.
tasks.fileSendChunkSize = 16 MiB

# Settings for the Amazon AWS interface.
# These are mostly in line with the LSF settings adapted to AWS.
tasks.elastic.aws {

      #fileStoreBucket = ""

      useS3FileStore = true

      endpoint = "http://ec2.us-east-1.amazonaws.com"

      spotPrice = 0.271

      # instance store 64 bit: ami-570f603e
      # for cluster compute nodes: ami-a73758ce
      ami = "ami-a73758ce"

      instanceType = "cc2.8xlarge"

      #securityGroup = ""

      #jarBucket = ""

      #jarObject = ""

      extraFilesFromS3 = []

      extraStartupScript = ""

      jvmMaxHeapFactor = 0.5

      extraJavaCommandline = ""

      keyName = "pityukeypair"

      iamRole = "pipeline"

      # cache and log files are uploaded to S3 in every ? sec
      uploadInterval = 360 sec

      placementGroup = ""

}

# Don't touch.
# Akka DeadlineFailureDetector
tasks.failuredetector {

	heartbeat-interval = 1000 ms

      # if heatbeats pause for this duration, the detector will pull alarm
      acceptable-heartbeat-pause = 600 s
}

######################
# Task subsystem end #
######################
